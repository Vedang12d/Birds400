{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ntrain_dir = \"../input/100-bird-species/train\"\nvalid_dir = \"../input/100-bird-species/valid\"\ntest_dir = \"../input/100-bird-species/test\"\nclasses = os.listdir(train_dir)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:17:53.430812Z","iopub.execute_input":"2022-08-18T01:17:53.431262Z","iopub.status.idle":"2022-08-18T01:17:53.437909Z","shell.execute_reply.started":"2022-08-18T01:17:53.431226Z","shell.execute_reply":"2022-08-18T01:17:53.436869Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\"\n)\n\nval_datagen = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)\nval_generator = val_datagen.flow_from_directory(\n    valid_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\"\n)\n\ntest_datagen = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:17:54.682343Z","iopub.execute_input":"2022-08-18T01:17:54.682726Z","iopub.status.idle":"2022-08-18T01:17:58.640756Z","shell.execute_reply.started":"2022-08-18T01:17:54.682695Z","shell.execute_reply":"2022-08-18T01:17:58.639695Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nbase_model = tensorflow.keras.applications.efficientnet.EfficientNetB4(\n    include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='max'\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:17:58.642885Z","iopub.execute_input":"2022-08-18T01:17:58.643289Z","iopub.status.idle":"2022-08-18T01:18:01.648118Z","shell.execute_reply.started":"2022-08-18T01:17:58.643250Z","shell.execute_reply":"2022-08-18T01:18:01.647132Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras import regularizers\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(BatchNormalization())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.45))\nmodel.add(Dense(400, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:03.603033Z","iopub.execute_input":"2022-08-18T01:18:03.603772Z","iopub.status.idle":"2022-08-18T01:18:05.162208Z","shell.execute_reply.started":"2022-08-18T01:18:03.603736Z","shell.execute_reply":"2022-08-18T01:18:05.161266Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:05.863020Z","iopub.execute_input":"2022-08-18T01:18:05.863653Z","iopub.status.idle":"2022-08-18T01:18:05.892422Z","shell.execute_reply.started":"2022-08-18T01:18:05.863613Z","shell.execute_reply":"2022-08-18T01:18:05.891332Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport time\nclass LRA(tf.keras.callbacks.Callback):\n    def __init__(self,model, base_model, patience,stop_patience, threshold, factor, dwell, batches, initial_epoch,epochs, ask_epoch, csv_path=None):\n        super(LRA, self).__init__()\n        self.model=model\n        self.base_model=base_model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience # specifies how many times to adjust lr without improvement to stop training\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.batches=batches # number of training batch to runn per epoch\n        self.initial_epoch=initial_epoch\n        self.epochs=epochs\n        self.ask_epoch=ask_epoch\n        self.ask_epoch_initial=ask_epoch # save this value to restore if restarting training\n        # self.csv_path=csv_path\n        # callback variables \n        self.count=0 # how many times lr has been reduced without improvement\n        self.stop_count=0        \n        self.best_epoch=1   # epoch with the lowest loss        \n        self.initial_lr=float(tf.keras.optimizers.Adam().lr.numpy()) # get the initiallearning rate and save it         \n        self.highest_tracc=0.0 # set highest training accuracy to 0 initially\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n        self.initial_weights=self.model.get_weights()   # save initial weights if they have to get restored \n        self.data_dict={}\n        for key in ['epoch','tr loss','tr acc','vloss','vacc','current lr','next lr','monitor','% improv','duration']:\n            self.data_dict[key]=[]\n    def on_train_begin(self, logs=None):        \n        if self.base_model != None:\n            status=base_model.trainable\n            if status:\n                msg=' initializing callback starting training with base_model trainable'\n            else:\n                msg='initializing callback starting training with base_model not trainable'\n        else:\n            msg='initialing callback and starting training'                        \n        print(msg) \n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n        print(msg) \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):\n        stop_time=time.time()\n        tr_duration= stop_time- self.start_time            \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        # if self.csv_path !=None:\n        #     df=pd.DataFrame.from_dict(self.data_dict)\n        #     now = datetime.now() \n        #     year = str(now.year)\n        #     month=str(now.month)\n        #     day=str(now.day)\n        #     hour=str(now.hour)\n        #     minute=str(now.minute)\n        #     sec=str(now.second)\n        #     label = month + '-'+ day + '-' + year + '-' + hour + '-' + minute + '-' + sec +'.csv'\n        #     csv_path=self.csv_path + '-'+ label\n        #     df.to_csv(csv_path, index=False)\n        #     print('training data saved as ', csv_path)\n\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n        msg=f'Training is completed - model is set with weights from epoch {self.best_epoch} '\n        print(msg)\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print(msg)   \n        \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs['accuracy']* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format('', str(batch), str(self.batches), acc, loss)\n#         print(msg, end='\\r') # prints over on the same line to show running batch count        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs['val_loss']  # get the validation loss for this epoch\n        acc=logs['accuracy']  # get training accuracy \n        v_acc=logs['val_accuracy']\n        loss=logs['loss']        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if epoch ==0:\n                pimprov=0.0\n            else:\n                pimprov= (acc-self.highest_tracc )*100/self.highest_tracc\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                self.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                # color= (0,255,0)\n                self.best_epoch=epoch + 1  # set the value of best epoch for this epoch              \n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1: # lr should be adjusted\n                    # color=(245, 170, 66)\n                    lr*=self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count+=1 # count the number of consecutive lr adjustments\n                    self.count=0 # reset counter\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space                        \n                    elif v_loss<self.lowest_vloss:\n                        self.lowest_vloss=v_loss                                    \n                else:\n                    self.count+=1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if epoch==0:\n                pimprov=0.0\n            else:\n                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                self.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                # color=(0,255,0)                \n                self.best_epoch=epoch + 1 # set the value of the best epoch to this epoch\n            else: # validation loss did not improve\n                if self.count>=self.patience-1: # need to adjust lr\n                    # color=(245, 170, 66)\n                    lr*=self.factor # adjust the learning rate                    \n                    self.stop_count+=1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space\n                else: \n                    self.count+=1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n        print(msg)\n        key_list=['epoch','tr loss','tr acc','vloss','vacc','current lr','next lr','monitor','% improv','duration']\n        val_list =[epoch + 1, loss, acc, v_loss, v_acc, current_lr, lr, monitor, pimprov, duration]\n        for key, value in zip(key_list, val_list):\n           self.data_dict[key].append(value)\n        \n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print(msg)\n            self.model.stop_training = True # stop training\n        else: \n            if self.ask_epoch !=None:\n                if epoch + 1 >= self.ask_epoch:\n                    if base_model.trainable:\n                        msg='enter H to halt training or an integer for number of epochs to run then ask again'\n                    else:\n                        msg='enter H to halt training ,F to fine tune model, or an integer for number of epochs to run then ask again'\n                    print(msg)\n                    ans=input('')                    \n                    if ans=='H' or ans=='h':\n                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n                        print(msg)\n                        self.model.stop_training = True # stop training\n                    elif ans == 'T' or ans=='t':\n                        if base_model.trainable:\n                            msg='base_model is already set as trainable'\n                        else:\n                            msg='setting base_model as trainable for fine tuning of model'\n                            self.base_model.trainable=True\n                        print(msg)\n                        msg='Enter an integer for the number of epochs to run then be asked again'\n                        print(msg)\n                        ans=int(input())\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch) \n                        print(msg)    \n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print(msg)                         \n                        self.count=0\n                        self.stop_count=0                        \n                        self.ask_epoch = epoch + 1 + self.ask_epoch_initial \n                        \n                    else:\n                        ans=int(ans)\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch)                         \n                        print(msg)\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print(msg) ","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:07.532143Z","iopub.execute_input":"2022-08-18T01:18:07.532501Z","iopub.status.idle":"2022-08-18T01:18:07.568667Z","shell.execute_reply.started":"2022-08-18T01:18:07.532469Z","shell.execute_reply":"2022-08-18T01:18:07.567552Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ntrain_steps=int(np.ceil(len(train_generator.labels)/32))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:09.390113Z","iopub.execute_input":"2022-08-18T01:18:09.391633Z","iopub.status.idle":"2022-08-18T01:18:09.396518Z","shell.execute_reply.started":"2022-08-18T01:18:09.391590Z","shell.execute_reply":"2022-08-18T01:18:09.395511Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\nstop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\nthreshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\nfactor=.5 # factor to reduce lr by\ndwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\nfreeze=False # if true free weights of  the base model\nask_epoch=20 # number of epochs to run before asking if you want to halt training\nbatches=train_steps\n# csv_path=os.path.join(working_dir,'my_csv')\ncallbacks=[LRA(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold, factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=40, ask_epoch=ask_epoch)]","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:13.789498Z","iopub.execute_input":"2022-08-18T01:18:13.790592Z","iopub.status.idle":"2022-08-18T01:18:14.201897Z","shell.execute_reply.started":"2022-08-18T01:18:13.790549Z","shell.execute_reply":"2022-08-18T01:18:14.200395Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nhistory = model.fit(\n    train_generator, epochs=40, callbacks=callbacks, validation_data=val_generator, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T01:18:18.480392Z","iopub.execute_input":"2022-08-18T01:18:18.481127Z","iopub.status.idle":"2022-08-18T07:57:17.568783Z","shell.execute_reply.started":"2022-08-18T01:18:18.481066Z","shell.execute_reply":"2022-08-18T07:57:17.567754Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val_loss\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history[\"accuracy\"], label=\"acc\")\nplt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-18T07:58:05.058597Z","iopub.execute_input":"2022-08-18T07:58:05.058979Z","iopub.status.idle":"2022-08-18T07:58:48.563872Z","shell.execute_reply.started":"2022-08-18T07:58:05.058949Z","shell.execute_reply":"2022-08-18T07:58:48.562984Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_generator)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-18T07:59:01.402652Z","iopub.execute_input":"2022-08-18T07:59:01.403315Z","iopub.status.idle":"2022-08-18T07:59:36.588498Z","shell.execute_reply.started":"2022-08-18T07:59:01.403274Z","shell.execute_reply":"2022-08-18T07:59:36.587577Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model2.evaluate(test_generator)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:40:28.945612Z","iopub.execute_input":"2022-08-18T10:40:28.946002Z","iopub.status.idle":"2022-08-18T10:40:56.502059Z","shell.execute_reply.started":"2022-08-18T10:40:28.945965Z","shell.execute_reply":"2022-08-18T10:40:56.500990Z"},"trusted":true},"execution_count":81,"outputs":[]}]}